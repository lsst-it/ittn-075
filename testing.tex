\section{Testing}

\subsection{Site to site reachability}
Once the tunnels were up, we start doing site to site ICMP tests to check if the both sites were reachable using the tunnels and jumbo frames. 

Note: As mentioned on the IPsec documentation, there's is a 82-bytes header to consider. 
82-bytes header + 8918-bytes  = 9K MTU 

    \begin{lstlisting}
sdc-a02-ipsec01#ping vrf VRF_PIXEL 10.77.77.0 source tunnel 101 df-bit size 8946
PING 10.77.77.0 (10.77.77.0) from 10.77.77.1 : 8918(8946) bytes of data.
8926 bytes from 10.77.77.0: icmp_seq=1 ttl=64 time=226 ms
8926 bytes from 10.77.77.0: icmp_seq=2 ttl=64 time=226 ms
8926 bytes from 10.77.77.0: icmp_seq=3 ttl=64 time=226 ms
8926 bytes from 10.77.77.0: icmp_seq=4 ttl=64 time=226 ms
8926 bytes from 10.77.77.0: icmp_seq=5 ttl=64 time=226 ms

--- 10.77.77.0 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 41ms
rtt min/avg/max/mdev = 226.414/226.433/226.486/0.026 ms, pipe 5, 
ipg/ewma 10.166/226.459 ms

sdc-a02-ipsec01#ping vrf VRF_PIXEL 10.77.77.2 source tunnel 102 df-bit size 8946
PING 10.77.77.2 (10.77.77.2) from 10.77.77.3 : 8918(8946) bytes of data.
8926 bytes from 10.77.77.2: icmp_seq=1 ttl=64 time=226 ms
8926 bytes from 10.77.77.2: icmp_seq=2 ttl=64 time=226 ms
8926 bytes from 10.77.77.2: icmp_seq=3 ttl=64 time=226 ms
8926 bytes from 10.77.77.2: icmp_seq=4 ttl=64 time=226 ms
8926 bytes from 10.77.77.2: icmp_seq=5 ttl=64 time=226 ms

sdc-a02-ipsec02#ping vrf VRF_PIXEL 10.77.77.4 source tunnel 103 df-bit size 8946
PING 10.77.77.4 (10.77.77.4) from 10.77.77.5 : 8918(8946) bytes of data.
8926 bytes from 10.77.77.4: icmp_seq=1 ttl=64 time=227 ms
8926 bytes from 10.77.77.4: icmp_seq=2 ttl=64 time=226 ms
8926 bytes from 10.77.77.4: icmp_seq=3 ttl=64 time=226 ms
8926 bytes from 10.77.77.4: icmp_seq=4 ttl=64 time=226 ms
8926 bytes from 10.77.77.4: icmp_seq=5 ttl=64 time=226 ms

--- 10.77.77.4 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 41ms
rtt min/avg/max/mdev = 226.401/226.468/226.560/0.055 ms, pipe 5, 
ipg/ewma 10.159/226.510 ms


sdc-a02-ipsec02#ping vrf VRF_PIXEL 10.77.77.6 source tunnel 104 df-bit size 8946
PING 10.77.77.6 (10.77.77.6) from 10.77.77.7 : 8918(8946) bytes of data.
8926 bytes from 10.77.77.6: icmp_seq=1 ttl=64 time=227 ms
8926 bytes from 10.77.77.6: icmp_seq=2 ttl=64 time=227 ms
8926 bytes from 10.77.77.6: icmp_seq=3 ttl=64 time=227 ms
8926 bytes from 10.77.77.6: icmp_seq=4 ttl=64 time=227 ms
8926 bytes from 10.77.77.6: icmp_seq=5 ttl=64 time=227 ms
\end{lstlisting}

\subsection{Routing Failover}

OSPF is configured to provide a routing failover when one of the physical interfaces or logical tunnels goes down.

The failover works as follows:

    \begin{enumerate}
        \item If Tunnel101 goes down, it will failover to Tunnel102, and continue to Tunnel103 if Tunnel102 goes down, until there's only Tunnel104 available.
        \item If one of the previous Tunnels goes back to normal, the routing table will be fill with the route of the Tunnel with the smaller ID value.
        \item Same behavior occurs if one of the physical links goes down.
    \end{enumerate}

This is how it looks when the primary link is up and run campaigns to encourage people to
    \begin{lstlisting}
    S   172.24.7.0/24 [1/0]
        via 10.77.77.0, Tunnel101, Static Interface IPsec tunnel index 101, 
        dst 134.79.22.230, src 139.229.140.1
    \end{lstlisting}

This is how it looks when the primary goes down and failover is engage.
    \begin{lstlisting}
    S   172.24.7.0/24 [1/0]
        via 10.77.77.2, Tunnel102, Static Interface IPsec tunnel index 102, 
        dst 134.79.22.226, src 139.229.140.3
    \end{lstlisting}

Note: 172.24.7.0/24 is the SLAC landing network.

\subsection{Performance}
The overall testing was made with two endpoint, one on each side behind the IPsec tunnels environment, each node connected using a 100Gbps NIC into the IPsec TOR switches.

Here's the Rubin endpoint TCP configuration for this node:

    \begin{lstlisting}
        [root@perfsonar1 sysctl.d]# cat 10-perfsonar.conf 
        # HEADER: This file was autogenerated at 2024-05-27 21:56:26 +0000
        # HEADER: by puppet.  While it can still be managed manually, it
        # HEADER: is definitely not recommended.
        net.core.rmem_max=268435456
        net.core.wmem_max=268435456
        net.ipv4.tcp_rmem=4096	87380	134217728
        net.ipv4.tcp_wmem=4096	65536	134217728
        net.ipv4.tcp_congestion_control=htcp
        net.core.default_qdisc=fq
        net.ipv4.conf.all.arp_announce=2
        net.ipv4.conf.all.arp_ignore=1
        net.ipv4.conf.all.arp_filter=1
        net.ipv4.conf.default.arp_filter=1
        net.ipv4.tcp_no_metrics_save=1
        \end{lstlisting}

The choosen tool for this test was iperf3. Here is the test results for 7 seconds and 60 seconds using above TCP configuration:

    \begin{lstlisting}
        [root@perfsonar1 sysctl.d]# iperf3 -c 172.24.7.31 -P 10 -t 7
        [SUM]  0.00-7.00  sec 12.8 GBytes 15.7 Gbits/sec  0      sender
        [SUM]  0.00-7.19  sec 12.8 GBytes 15.3 Gbits/sec         receiver
        iperf Done.

        [root@perfsonar1 sysctl.d]# iperf3 -c 172.24.7.31 -P 10 -t 60
        [SUM]  0.00-60.00 sec  138 GBytes 19.8 Gbits/sec 67384   sender
        [SUM]  0.00-60.19 sec  138 GBytes 19.7 Gbits/sec         receiver
        iperf Done.
    \end{lstlisting}

\newpage

Then we used another TCP congestion protocol: BBR 

    \begin{lstlisting}
        sysctl net.ipv4.tcp_congestion_control=bbr
    \end{lstlisting}

To learn more about BBR, please consider to check this document \href{https://internet2.edu/wp-content/uploads/2022/12/techex22-AdvancedNetworking-ExploringtheBBRv2CongestionControlAlgorithm-Tierney.pdf}{Exploring the BBRv2 Congestion Control
Algorithm for use on Data Transfer Nodes}

The Rubin and SLAC endpoints were set to use BBR as IPv4 TCP congestion protocol and this was the result for 7 seconds and 60 seconds:

    \begin{lstlisting}
    [root@perfsonar1 ~]# ./iperf3-static -c 172.24.7.31 -P 64 -t 7 -w 100M
    [SUM]  0.00-7.01  sec 36.1 GBytes 44.2 Gbits/sec 420779       sender
    [SUM]  0.00-7.24  sec 29.0 GBytes 34.4 Gbits/sec              receiver
    iperf Done.

    
    [root@perfsonar1 ~]# ./iperf3-static -c 172.24.7.31 -P 64 -t 60 -w 100M
    [SUM]  0.00-60.01 sec  342 GBytes 48.9 Gbits/sec 3028493       sender
    [SUM]  0.00-60.24 sec  332 GBytes 47.4 Gbits/sec         receiver
    iperf Done.
    \end{lstlisting}

    